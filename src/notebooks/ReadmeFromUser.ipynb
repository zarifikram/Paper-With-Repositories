{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bba688a-5491-43d2-84d9-1bd275b99959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current working dir/Volumes/Zarif/myStuff/research/repoPaper/src\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os, sys\n",
    "exec(open(\"init_notebook.py\").read())\n",
    "from DataTools import DataTools\n",
    "from RepoTools import RepoTools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2f594bd-287f-41ed-a0fe-a74e3198b9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea6598c2-6066-4244-ac2f-41fa763d3cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                            | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page: 0/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█████████████▌                                                      | 1/5 [00:00<00:02,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page: 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███████████████████████████▏                                        | 2/5 [00:01<00:02,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page: 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|████████████████████████████████████████▊                           | 3/5 [00:02<00:01,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page: 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|██████████████████████████████████████████████████████▍             | 4/5 [00:03<00:00,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page: 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.35it/s]\n"
     ]
    }
   ],
   "source": [
    "user = RepoTools.github.get_user(\"amzn\")\n",
    "repos = RepoTools.getReposFromUser(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "51d85ff2-40c1-4637-8f63-825791afd6ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████| 132/132 [01:02<00:00,  2.12it/s]\n"
     ]
    }
   ],
   "source": [
    "repoWithReadmes = RepoTools.getRepoWithReadmes(repos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "369c489b-542f-441e-9f4e-1a79ff6199e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "RepoTools.saveAsPickle(repoWithReadmes, \"amazonRepoWithReadMe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "768cfb6a-3c25-45a0-a30f-eb3d1117ef2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "repoWithReadmes = RepoTools.loadPickle(\"amazonRepoWithReadMe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2c1b076e-02e1-4a1b-befc-6344405fc95d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{SDR}: Efficient Neural Re-ranking using Succinct DocumentRepresentation\n",
      "Studying the Effectiveness of\\xc2\\xa0Conversational Search Refinement Through User Simulation\n",
      "A First Look: Towards Explainable {T}ext{VQA} Models via Visual and Textual Explanations\n",
      "Misspelling Detection from Noisy Product Images\n",
      "{C}ycle{KQR}: Unsupervised Bidirectional Keyword-Question Rewriting\n",
      "Answering Ambiguous Questions through Generative Evidence Fusion and Round-Trip Prediction\n",
      "&quot;Pascal&quot;\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# bibtex_pattern = r'@\\w+\\{(.*?)\\}'\n",
    "# title_pattern = r'\\[(.*?)\\b(paper|Paper)\\b(.*?)\\]'\n",
    "tp2 = r'title={\\s*(\\S[^\\n\\r\\}]*)'\n",
    "tp1 = r'title\\s*=\\s*\"([^\"]+\\s*)+\"'\n",
    "\n",
    "for i, repoWithReadme in enumerate(repoWithReadmes):\n",
    "    rm = str(repoWithReadme[\"readme\"].decoded_content)\n",
    "    \n",
    "    # bibtex_matches = re.findall(bibtex_pattern, rm, re.DOTALL)\n",
    "    title_matches = re.findall(tp1, rm)\n",
    "    titles = [match.strip() for match in title_matches]\n",
    "    # if len(bibtex_matches) > 0:\n",
    "    #     print(bibtex_matches)\n",
    "    \n",
    "    if len(title_matches) > 0:\n",
    "        print(titles[0])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "20d3b4de-bbd2-44f1-a511-0e27bdaced52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named Entity Recognition with Small Strongly Labeled and Large Weakly Labeled Data\n",
      "Learning to Bid with AuctionGym\n",
      "Efficient Learning on Point Clouds With Basis Point Sets\n",
      "ScrabbleGAN: Semi-Supervised Varying Length Handwritten Text Generation\n",
      "Debiased balanced interleaving at Amazon Search\n",
      "Automatic Discovery of Privacy--Utility Pareto Fronts\n",
      "Learning Attribute-driven Disentangled Representations for Interactive Fashion Retrieval\n",
      "Revamping Cross-Modal Recipe Retrieval with Hierarchical Transformers and Self-supervised Learning\n",
      "Learning Multimodal Affinities for Textual Editing in Images\n",
      "RETE: Retrieval-Enhanced Temporal Event Forecasting on Unified Query Product Evolutionary Graph\n",
      "Multilingual Knowledge Graph Completion with Self-Supervised\\nAdaptive Graph Alignment\n",
      "Transformer uncertainty estimation with hierarchical stochastic attention\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# bibtex_pattern = r'@\\w+\\{(.*?)\\}'\n",
    "# title_pattern = r'\\[(.*?)\\b(paper|Paper)\\b(.*?)\\]'\n",
    "tp2 = r'title\\s*=\\s*{([^}]+\\s*)+}'\n",
    "# tp1 = r'title\\s*=\\s*\"([^\"]+\\s*)+\"'\n",
    "\n",
    "for i, repoWithReadme in enumerate(repoWithReadmes):\n",
    "    rm = str(repoWithReadme[\"readme\"].decoded_content)\n",
    "    \n",
    "    # bibtex_matches = re.findall(bibtex_pattern, rm, re.DOTALL)\n",
    "    title_matches = re.findall(tp2, rm)\n",
    "    titles = [match.strip() for match in title_matches]\n",
    "    # if len(bibtex_matches) > 0:\n",
    "    #     print(bibtex_matches)\n",
    "    \n",
    "    if len(title_matches) > 0:\n",
    "        print(titles[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "29d50dcb-16f0-4ca5-8d31-8625c0634d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://github.com/amzn/amazon-succinct-doc-representation\n",
      "https://github.com/amzn/amazon-weak-ner-needle\n",
      "https://github.com/amzn/auction-gym\n",
      "https://github.com/amzn/basis-point-sets\n",
      "https://github.com/amzn/convolutional-handwriting-gan\n",
      "https://github.com/amzn/debiased-balanced-interleaving\n",
      "https://github.com/amzn/differential-privacy-bayesian-optimization\n",
      "https://github.com/amzn/explainable-text-vqa\n",
      "https://github.com/amzn/fashion-attribute-disentanglement\n",
      "https://github.com/amzn/image-misspell-coling2020\n",
      "https://github.com/amzn/image-to-recipe-transformers\n",
      "https://github.com/amzn/kqr\n",
      "https://github.com/amzn/multimodal-affinities\n",
      "https://github.com/amzn/refuel-open-domain-qa\n",
      "https://github.com/amzn/rete-thewebconf-2022\n",
      "https://github.com/amzn/ss-aga-kgc\n",
      "https://github.com/amzn/sto-transformer\n"
     ]
    }
   ],
   "source": [
    "for i, repoWithReadme in enumerate(repoWithReadmes):\n",
    "    rm = str(repoWithReadme[\"readme\"].decoded_content)\n",
    "    \n",
    "    if '@inproceedings' in rm or '@article' in rm or '@misc' in rm:\n",
    "        print('https://github.com/' + repoWithReadme['repo'].full_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b1c03cb9-209b-4c78-821b-673d4b26d43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "title_patterns = [r'title\\s*=\\s*{([^}]+\\s*)+}', r'title\\s*=\\s*\"([^\"]+\\s*)+\"']\n",
    "paperWithRepoURLs = []\n",
    "for repoWithReadme in enumerate(repoWithReadmes):\n",
    "    rm = str(repoWithReadme[\"readme\"].decoded_content.decode(\"utf-8\"))\n",
    "    repoName = repoWithReadme['repo'].full_name\n",
    "    for title_pattern in title_patterns:\n",
    "        title_matches = re.findall(title_pattern, rm)\n",
    "        titles = [match.strip() for match in title_matches]\n",
    "        if len(titles) > 0 and len(titles[0]) >0:\n",
    "            paperWithRepoURL = {\"title\":titles[0], \"url\":\"https://github.com/\"+repoName}\n",
    "            paperWithRepoURLs.append(paperWithRepoURL)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "05ffb44a-d0b6-47bc-b7d7-6bdb2605a3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "paperDF = RepoTools.getPaperWithRepoDfFromREADME(repoWithReadmes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "f2adb2df-8e5a-449a-a145-97b06bbea6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "DataTools.saveDfInCSV(paperDF, \"amazonPapers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "9f8bdac2-d8a8-43d4-a90e-e2b61b0a481d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                           | 0/12 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page: 0/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|█████▌                                                             | 1/12 [00:00<00:09,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page: 1/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|███████████▏                                                       | 2/12 [00:01<00:08,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page: 2/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|████████████████▊                                                  | 3/12 [00:02<00:07,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page: 3/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|██████████████████████▎                                            | 4/12 [00:03<00:06,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page: 4/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|███████████████████████████▉                                       | 5/12 [00:04<00:05,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page: 5/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████▌                                 | 6/12 [00:04<00:04,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page: 6/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|███████████████████████████████████████                            | 7/12 [00:05<00:04,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page: 7/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|████████████████████████████████████████████▋                      | 8/12 [00:06<00:03,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page: 8/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|██████████████████████████████████████████████████▎                | 9/12 [00:07<00:02,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page: 9/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|███████████████████████████████████████████████████████           | 10/12 [00:08<00:01,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page: 10/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|████████████████████████████████████████████████████████████▌     | 11/12 [00:09<00:00,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page: 11/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 12/12 [00:09<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching the readmes from the repos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████| 352/352 [02:50<00:00,  2.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching the extra github links and readmes from the repoWithReadme\n",
      "Found more than 5 github links in the readme. Ignoring the links\n",
      "Found more than 5 github links in the readme. Ignoring the links\n",
      "Found more than 5 github links in the readme. Ignoring the links\n",
      "Found more than 5 github links in the readme. Ignoring the links\n",
      "Found more than 5 github links in the readme. Ignoring the links\n",
      "Found more than 5 github links in the readme. Ignoring the links\n",
      "Found more than 5 github links in the readme. Ignoring the links\n",
      "Found more than 5 github links in the readme. Ignoring the links\n",
      "Found more than 5 github links in the readme. Ignoring the links\n",
      "Found more than 5 github links in the readme. Ignoring the links\n",
      "Found more than 5 github links in the readme. Ignoring the links\n",
      "Found more than 5 github links in the readme. Ignoring the links\n",
      "Found more than 5 github links in the readme. Ignoring the links\n",
      "Found more than 5 github links in the readme. Ignoring the links\n",
      "Found more than 5 github links in the readme. Ignoring the links\n",
      "Found more than 5 github links in the readme. Ignoring the links\n",
      "Found more than 5 github links in the readme. Ignoring the links\n",
      "Found more than 5 github links in the readme. Ignoring the links\n",
      "Found more than 5 github links in the readme. Ignoring the links\n",
      "Found more than 5 github links in the readme. Ignoring the links\n",
      "Found more than 5 github links in the readme. Ignoring the links\n",
      "Found more than 5 github links in the readme. Ignoring the links\n",
      "Found more than 5 github links in the readme. Ignoring the links\n",
      "Found more than 5 github links in the readme. Ignoring the links\n",
      "Found more than 5 github links in the readme. Ignoring the links\n",
      "Found more than 5 github links in the readme. Ignoring the links\n",
      "Found more than 5 github links in the readme. Ignoring the links\n",
      "Found more than 5 github links in the readme. Ignoring the links\n",
      "Found more than 5 github links in the readme. Ignoring the links\n",
      "Found more than 5 github links in the readme. Ignoring the links\n",
      "Found more than 5 github links in the readme. Ignoring the links\n",
      "Found more than 5 github links in the readme. Ignoring the links\n",
      "Found more than 5 github links in the readme. Ignoring the links\n",
      "Found more than 5 github links in the readme. Ignoring the links\n",
      "Found more than 5 github links in the readme. Ignoring the links\n",
      "Found more than 5 github links in the readme. Ignoring the links\n",
      "Found more than 5 github links in the readme. Ignoring the links\n",
      "Found more than 5 github links in the readme. Ignoring the links\n",
      "Found more than 5 github links in the readme. Ignoring the links\n",
      "Found more than 5 github links in the readme. Ignoring the links\n",
      "Found more than 5 github links in the readme. Ignoring the links\n",
      "Found more than 5 github links in the readme. Ignoring the links\n",
      "Found more than 5 github links in the readme. Ignoring the links\n",
      "Found more than 5 github links in the readme. Ignoring the links\n",
      "Found more than 5 github links in the readme. Ignoring the links\n",
      "Found more than 5 github links in the readme. Ignoring the links\n",
      "Found more than 5 github links in the readme. Ignoring the links\n",
      "Found more than 5 github links in the readme. Ignoring the links\n",
      "Found more than 5 github links in the readme. Ignoring the links\n",
      "Found more than 5 github links in the readme. Ignoring the links\n",
      "Found more than 5 github links in the readme. Ignoring the links\n",
      "Found more than 5 github links in the readme. Ignoring the links\n",
      "Found more than 5 github links in the readme. Ignoring the links\n",
      "Found more than 5 github links in the readme. Ignoring the links\n",
      "Found more than 5 github links in the readme. Ignoring the links\n",
      "Found more than 5 github links in the readme. Ignoring the links\n",
      "Found more than 5 github links in the readme. Ignoring the links\n",
      "Found more than 5 github links in the readme. Ignoring the links\n",
      "Found more than 5 github links in the readme. Ignoring the links\n",
      "Found more than 5 github links in the readme. Ignoring the links\n",
      "Found more than 5 github links in the readme. Ignoring the links\n",
      "Found more than 5 github links in the readme. Ignoring the links\n",
      "Found more than 5 github links in the readme. Ignoring the links\n",
      "Found more than 5 github links in the readme. Ignoring the links\n",
      "Found more than 5 github links in the readme. Ignoring the links\n",
      "Found more than 5 github links in the readme. Ignoring the links\n",
      "Found more than 5 github links in the readme. Ignoring the links\n",
      "Found more than 5 github links in the readme. Ignoring the links\n",
      "Found more than 5 github links in the readme. Ignoring the links\n",
      "Found more than 5 github links in the readme. Ignoring the links\n",
      "Found more than 5 github links in the readme. Ignoring the links\n",
      "Found more than 5 github links in the readme. Ignoring the links\n",
      "Found more than 5 github links in the readme. Ignoring the links\n",
      "Found more than 5 github links in the readme. Ignoring the links\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|██▉                                                               | 9/201 [00:04<01:30,  2.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find repository for this link : JuliaGPU/AMGX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███████████████████▋                                             | 61/201 [00:30<01:08,  2.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find repository for this link : NVIDIA/energy-sdk-rapids-seismic-facies\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|███████████████████████████████████████▍                        | 124/201 [01:04<00:38,  2.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find repository for this link : askap-benchmarks/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████████████████████████████████████████                      | 132/201 [01:08<00:35,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find repository for this link : cpm-cmake/CPM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|██████████████████████████████████████████████████▋             | 159/201 [01:24<00:19,  2.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find repository for this link : mrdoob/three\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|███████████████████████████████████████████████████████▍        | 174/201 [01:32<00:13,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find repository for this link : nvidia/triton\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████| 201/201 [01:46<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching the readmes from the repos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 85/85 [00:40<00:00,  2.12it/s]\n",
      " 34%|█████████████████████▋                                          | 141/415 [00:17<00:44,  6.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Too many arxiv links (10), skipping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|███████████████████████████████████████████████████████▉        | 363/415 [01:11<00:10,  4.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Too many arxiv links (10), skipping\n",
      "Too many arxiv links (10), skipping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████| 415/415 [01:12<00:00,  5.72it/s]\n"
     ]
    }
   ],
   "source": [
    "extendedData = RepoTools.savePaperNameFromGithubUser(\"NVIDIA\", saveRepoWithReadme=True, saveExtendedRepoWithReadme=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "13f6040f-a6b1-416b-bbc9-309e76f3aeac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|████████████████████████████████▋                               | 221/432 [04:46<10:22,  2.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Too many arxiv links, skipping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████████████████████████████████████████████████▍            | 347/432 [07:40<02:21,  1.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Too many arxiv links, skipping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████████████████████████████████████████████████████▉       | 384/432 [08:22<00:53,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Too many arxiv links, skipping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████| 432/432 [09:11<00:00,  1.28s/it]\n"
     ]
    }
   ],
   "source": [
    "extendedData = RepoTools.loadReposWithReadmeAndGetExtendedData('NVlabsRepoWithReadmes', saveExtendedRepoWithReadme=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "f9c93673-9768-4d7e-8b29-d4a8fe66cd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = RepoTools.loadExtendedRepoWithReadmeAndGetPaperDf(\"extended_NVIDIARepoWithReadmes\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "85279612-99e8-4b87-9678-5c109e6e3fce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>repo_link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{NDDS}: {NVIDIA} Deep Learning Dataset Synthes...</td>\n",
       "      <td>https://github.com/NVIDIA/Dataset_Synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{NeMo (Inverse) Text Normalization: From Devel...</td>\n",
       "      <td>https://github.com/NVIDIA/NeMo-text-processing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NVVL: NVIDIA Video Loader</td>\n",
       "      <td>https://github.com/NVIDIA/nvvl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>High-Resolution Image Synthesis and Semantic M...</td>\n",
       "      <td>https://github.com/NVIDIA/pix2pixHD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NVIDIA Rivermax SDK</td>\n",
       "      <td>https://github.com/NVIDIA/Rivermax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>Deep Residual Learning for Image Recognition</td>\n",
       "      <td>https://github.com/NVIDIA/retinanet-examples</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>4D Spatio-Temporal ConvNets: Minkowski Convolu...</td>\n",
       "      <td>https://github.com/NVIDIA/MinkowskiEngine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>Fully Convolutional Geometric Features</td>\n",
       "      <td>https://github.com/NVIDIA/MinkowskiEngine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>High-dimensional Convolutional Networks for Ge...</td>\n",
       "      <td>https://github.com/NVIDIA/MinkowskiEngine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>Generative Sparse Detection Networks for 3D Si...</td>\n",
       "      <td>https://github.com/NVIDIA/MinkowskiEngine</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>62 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title  \\\n",
       "0   {NDDS}: {NVIDIA} Deep Learning Dataset Synthes...   \n",
       "1   {NeMo (Inverse) Text Normalization: From Devel...   \n",
       "2                           NVVL: NVIDIA Video Loader   \n",
       "3   High-Resolution Image Synthesis and Semantic M...   \n",
       "4                                 NVIDIA Rivermax SDK   \n",
       "..                                                ...   \n",
       "57       Deep Residual Learning for Image Recognition   \n",
       "58  4D Spatio-Temporal ConvNets: Minkowski Convolu...   \n",
       "59             Fully Convolutional Geometric Features   \n",
       "60  High-dimensional Convolutional Networks for Ge...   \n",
       "61  Generative Sparse Detection Networks for 3D Si...   \n",
       "\n",
       "                                         repo_link  \n",
       "0    https://github.com/NVIDIA/Dataset_Synthesizer  \n",
       "1   https://github.com/NVIDIA/NeMo-text-processing  \n",
       "2                   https://github.com/NVIDIA/nvvl  \n",
       "3              https://github.com/NVIDIA/pix2pixHD  \n",
       "4               https://github.com/NVIDIA/Rivermax  \n",
       "..                                             ...  \n",
       "57    https://github.com/NVIDIA/retinanet-examples  \n",
       "58       https://github.com/NVIDIA/MinkowskiEngine  \n",
       "59       https://github.com/NVIDIA/MinkowskiEngine  \n",
       "60       https://github.com/NVIDIA/MinkowskiEngine  \n",
       "61       https://github.com/NVIDIA/MinkowskiEngine  \n",
       "\n",
       "[62 rows x 2 columns]"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "dc282b6f-f980-4660-ac22-47263b8b4fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "DataTools.saveDfInCSV(df, 'NVIDIAPapers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "9a7454d0-6608-4a0f-a80a-d93dc6cba3f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[df.repo_link.str.contains('NVIDIA')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "d8a904e6-202d-4883-b817-bbf8cfb9c70e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One weird trick for parallelizing convolutional neural networks'"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08faff4e-ef75-4a7a-ba25-26c876999abb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
