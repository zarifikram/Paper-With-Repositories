{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bba688a-5491-43d2-84d9-1bd275b99959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current working dir/Volumes/Zarif/myStuff/research/repoPaper/src\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os, sys\n",
    "exec(open(\"init_notebook.py\").read())\n",
    "from DataTools import DataTools\n",
    "from RepoTools import RepoTools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2f594bd-287f-41ed-a0fe-a74e3198b9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea6598c2-6066-4244-ac2f-41fa763d3cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                            | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page: 0/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█████████████▌                                                      | 1/5 [00:00<00:02,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page: 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███████████████████████████▏                                        | 2/5 [00:01<00:02,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page: 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|████████████████████████████████████████▊                           | 3/5 [00:02<00:01,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page: 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|██████████████████████████████████████████████████████▍             | 4/5 [00:03<00:00,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page: 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.35it/s]\n"
     ]
    }
   ],
   "source": [
    "user = RepoTools.github.get_user(\"amzn\")\n",
    "repos = RepoTools.getReposFromUser(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "51d85ff2-40c1-4637-8f63-825791afd6ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████| 132/132 [01:02<00:00,  2.12it/s]\n"
     ]
    }
   ],
   "source": [
    "repoWithReadmes = RepoTools.getRepoWithReadmes(repos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "369c489b-542f-441e-9f4e-1a79ff6199e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "RepoTools.saveAsPickle(repoWithReadmes, \"amazonRepoWithReadMe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "768cfb6a-3c25-45a0-a30f-eb3d1117ef2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "repoWithReadmes = RepoTools.loadPickle(\"amazonRepoWithReadMe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2c1b076e-02e1-4a1b-befc-6344405fc95d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{SDR}: Efficient Neural Re-ranking using Succinct DocumentRepresentation\n",
      "Studying the Effectiveness of\\xc2\\xa0Conversational Search Refinement Through User Simulation\n",
      "A First Look: Towards Explainable {T}ext{VQA} Models via Visual and Textual Explanations\n",
      "Misspelling Detection from Noisy Product Images\n",
      "{C}ycle{KQR}: Unsupervised Bidirectional Keyword-Question Rewriting\n",
      "Answering Ambiguous Questions through Generative Evidence Fusion and Round-Trip Prediction\n",
      "&quot;Pascal&quot;\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# bibtex_pattern = r'@\\w+\\{(.*?)\\}'\n",
    "# title_pattern = r'\\[(.*?)\\b(paper|Paper)\\b(.*?)\\]'\n",
    "tp2 = r'title={\\s*(\\S[^\\n\\r\\}]*)'\n",
    "tp1 = r'title\\s*=\\s*\"([^\"]+\\s*)+\"'\n",
    "\n",
    "for i, repoWithReadme in enumerate(repoWithReadmes):\n",
    "    rm = str(repoWithReadme[\"readme\"].decoded_content)\n",
    "    \n",
    "    # bibtex_matches = re.findall(bibtex_pattern, rm, re.DOTALL)\n",
    "    title_matches = re.findall(tp1, rm)\n",
    "    titles = [match.strip() for match in title_matches]\n",
    "    # if len(bibtex_matches) > 0:\n",
    "    #     print(bibtex_matches)\n",
    "    \n",
    "    if len(title_matches) > 0:\n",
    "        print(titles[0])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "20d3b4de-bbd2-44f1-a511-0e27bdaced52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named Entity Recognition with Small Strongly Labeled and Large Weakly Labeled Data\n",
      "Learning to Bid with AuctionGym\n",
      "Efficient Learning on Point Clouds With Basis Point Sets\n",
      "ScrabbleGAN: Semi-Supervised Varying Length Handwritten Text Generation\n",
      "Debiased balanced interleaving at Amazon Search\n",
      "Automatic Discovery of Privacy--Utility Pareto Fronts\n",
      "Learning Attribute-driven Disentangled Representations for Interactive Fashion Retrieval\n",
      "Revamping Cross-Modal Recipe Retrieval with Hierarchical Transformers and Self-supervised Learning\n",
      "Learning Multimodal Affinities for Textual Editing in Images\n",
      "RETE: Retrieval-Enhanced Temporal Event Forecasting on Unified Query Product Evolutionary Graph\n",
      "Multilingual Knowledge Graph Completion with Self-Supervised\\nAdaptive Graph Alignment\n",
      "Transformer uncertainty estimation with hierarchical stochastic attention\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# bibtex_pattern = r'@\\w+\\{(.*?)\\}'\n",
    "# title_pattern = r'\\[(.*?)\\b(paper|Paper)\\b(.*?)\\]'\n",
    "tp2 = r'title\\s*=\\s*{([^}]+\\s*)+}'\n",
    "# tp1 = r'title\\s*=\\s*\"([^\"]+\\s*)+\"'\n",
    "\n",
    "for i, repoWithReadme in enumerate(repoWithReadmes):\n",
    "    rm = str(repoWithReadme[\"readme\"].decoded_content)\n",
    "    \n",
    "    # bibtex_matches = re.findall(bibtex_pattern, rm, re.DOTALL)\n",
    "    title_matches = re.findall(tp2, rm)\n",
    "    titles = [match.strip() for match in title_matches]\n",
    "    # if len(bibtex_matches) > 0:\n",
    "    #     print(bibtex_matches)\n",
    "    \n",
    "    if len(title_matches) > 0:\n",
    "        print(titles[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "29d50dcb-16f0-4ca5-8d31-8625c0634d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://github.com/amzn/amazon-succinct-doc-representation\n",
      "https://github.com/amzn/amazon-weak-ner-needle\n",
      "https://github.com/amzn/auction-gym\n",
      "https://github.com/amzn/basis-point-sets\n",
      "https://github.com/amzn/convolutional-handwriting-gan\n",
      "https://github.com/amzn/debiased-balanced-interleaving\n",
      "https://github.com/amzn/differential-privacy-bayesian-optimization\n",
      "https://github.com/amzn/explainable-text-vqa\n",
      "https://github.com/amzn/fashion-attribute-disentanglement\n",
      "https://github.com/amzn/image-misspell-coling2020\n",
      "https://github.com/amzn/image-to-recipe-transformers\n",
      "https://github.com/amzn/kqr\n",
      "https://github.com/amzn/multimodal-affinities\n",
      "https://github.com/amzn/refuel-open-domain-qa\n",
      "https://github.com/amzn/rete-thewebconf-2022\n",
      "https://github.com/amzn/ss-aga-kgc\n",
      "https://github.com/amzn/sto-transformer\n"
     ]
    }
   ],
   "source": [
    "for i, repoWithReadme in enumerate(repoWithReadmes):\n",
    "    rm = str(repoWithReadme[\"readme\"].decoded_content)\n",
    "    \n",
    "    if '@inproceedings' in rm or '@article' in rm or '@misc' in rm:\n",
    "        print('https://github.com/' + repoWithReadme['repo'].full_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b1c03cb9-209b-4c78-821b-673d4b26d43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "title_patterns = [r'title\\s*=\\s*{([^}]+\\s*)+}', r'title\\s*=\\s*\"([^\"]+\\s*)+\"']\n",
    "paperWithRepoURLs = []\n",
    "for repoWithReadme in enumerate(repoWithReadmes):\n",
    "    rm = str(repoWithReadme[\"readme\"].decoded_content.decode(\"utf-8\"))\n",
    "    repoName = repoWithReadme['repo'].full_name\n",
    "    for title_pattern in title_patterns:\n",
    "        title_matches = re.findall(title_pattern, rm)\n",
    "        titles = [match.strip() for match in title_matches]\n",
    "        if len(titles) > 0 and len(titles[0]) >0:\n",
    "            paperWithRepoURL = {\"title\":titles[0], \"url\":\"https://github.com/\"+repoName}\n",
    "            paperWithRepoURLs.append(paperWithRepoURL)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "05ffb44a-d0b6-47bc-b7d7-6bdb2605a3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "paperDF = RepoTools.getPaperWithRepoDfFromREADME(repoWithReadmes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "f2adb2df-8e5a-449a-a145-97b06bbea6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "DataTools.saveDfInCSV(paperDF, \"amazonPapers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "9f8bdac2-d8a8-43d4-a90e-e2b61b0a481d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                            | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page: 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████████████████                                  | 1/2 [00:00<00:00,  1.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page: 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.18it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████| 55/55 [00:25<00:00,  2.19it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████| 54/54 [01:18<00:00,  1.46s/it]\n"
     ]
    }
   ],
   "source": [
    "extendedData = RepoTools.savePaperNameFromGithubUser(\"sony\", saveRepoWithReadme=True, saveExtendedRepoWithReadme=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "4d6a56b1-06bc-4de0-b0bf-ba5ab6e4a2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "paperTitles = []\n",
    "repoLinks = []\n",
    "\n",
    "sortedData = sorted(extendedData, key = lambda x : len(x['arxivPaperTitles']))\n",
    "\n",
    "for _ in sortedData:\n",
    "    repoLink = 'https://github.com/' + _['repo'].full_name\n",
    "    for title in _['papers']:\n",
    "        if not title in paperTitles:\n",
    "            paperTitles.append(title)\n",
    "            repoLinks.append(repoLink)\n",
    "\n",
    "\n",
    "    for title in _['arxivPaperTitles']:\n",
    "        if not title in paperTitles:\n",
    "            paperTitles.append(title)\n",
    "            repoLinks.append(repoLink)\n",
    "            \n",
    "df = pd.DataFrame()\n",
    "df['title'] = paperTitles\n",
    "df['link'] = repoLinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "d1d41617-5a0e-41e4-814d-a829aa9e87e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "DataTools.saveDfInCSV(df, 'sonyPapers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "71eec263-2229-46f0-bbde-c18a87e4e6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "repos = RepoTools.loadPickle('sonyRepoWithReadmes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "d65a633a-c603-4993-844a-351bfa5e2896",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'b\\'<head>\\\\n<link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\">\\\\n<link rel=\"stylesheet\" href=\"local.css\">\\\\n</head>\\\\n\\\\n<div class=\"bk_container\">\\\\n\\\\t<div class=\"bk_item\"><a href=\"#sec_dgm\">I. Deep Generative Modeling</a></div>\\\\n\\\\t<div class=\"bk_item\"><a href=\"#sec_nlp\">II. Multimodal NLP & Commonsense AI</a></div>\\\\n\\\\t<div class=\"bk_item\"><a href=\"#sec_mct\">III. Music & Cinematic Technologies</a></div>\\\\n</div>\\\\n\\\\n<a name=\"sec_dgm\"></a>\\\\n# I. Deep Generative Modeling \\\\n\\\\n<br>\\\\n\\\\n<div class=\"trow\">\\\\n\\\\t<div class=\"tile\">\\\\n\\\\t\\\\t<h3>SQ-VAE</h3>\\\\n\\\\t\\\\t<a href=\"https://proceedings.mlr.press/v162/takida22a.html\"><img src=\"./assets/sqvae.png\"></a>\\\\n\\\\t\\\\t<h5>\\\\n\\\\t\\\\t\\\\t<a href=\"https://proceedings.mlr.press/v162/takida22a.html\">[PMLR]</a>\\\\n\\\\t\\\\t\\\\t<a href=\"https://arxiv.org/abs/2205.07547\">[arXiv]</a>\\\\n\\\\t\\\\t\\\\t<a href=\"https://github.com/sony/sqvae\">[code]</a>\\\\n\\\\t\\\\t</h5>\\\\n\\\\t\\\\t<p>SQ-VAE: Variational Bayes on Discrete Representation with Self-annealed Stochastic Quantization<br>(ICML2022)</p>\\\\n\\\\t</div>\\\\n\\\\t<div class=\"tile\">\\\\n\\\\t\\\\t<h3>FP-Diffusion</h3>\\\\n\\\\t\\\\t<img src=\"./assets/ScoreFPE_3Doutline_single.gif\">\\\\n\\\\t\\\\t<h5>\\\\n\\\\t\\\\t\\\\t<a href=\"https://arxiv.org/abs/2210.04296\">[arXiv]</a>\\\\n\\\\t\\\\t</h5>\\\\n\\\\t\\\\t<p>Improving Score-based Diffusion Models by Enforcing the Underlying Score Fokker-Planck Equation (previous work was to NeurIPS2022 Workshop on Score-Based Methods)<br>(ICML2023)</p>\\\\n\\\\t\\\\t<div class=\"tile_highlight\">ICML23</div>\\\\n\\\\t</div>\\\\n\\\\t<div class=\"tile\">\\\\n\\\\t\\\\t<h3>GibbsDDRM</h3>\\\\n\\\\t\\\\t<img src=\"./assets/GibbsDDRM.png\">\\\\n\\\\t\\\\t<h5>\\\\n\\\\t\\\\t\\\\t<a href=\"https://arxiv.org/abs/2301.12686\">[arXiv]</a>\\\\n\\\\t\\\\t</h5>\\\\n\\\\t\\\\t<p>GibbsDDRM: A Partially Collapsed Gibbs Sampler for Solving Blind Linear Inverse Problems with Denoising Diffusion Restoration<br>(ICML2023)</p>\\\\n\\\\t\\\\t<div class=\"tile_highlight\">ICML23</div>\\\\n\\\\t</div>\\\\n\\\\t<div class=\"tile\">\\\\n\\\\t\\\\t<h3>Adversarially Slicing Generative Networks</h3>\\\\n\\\\t\\\\t<img src=\"./assets/ASGN.png\">\\\\n\\\\t\\\\t<h5>\\\\n\\\\t\\\\t\\\\t<a href=\"https://arxiv.org/abs/2301.12811\">[arXiv]</a>\\\\n\\\\t\\\\t</h5>\\\\n\\\\t\\\\t<p>Adversarially Slicing Generative Networks: Discriminator Slices Feature for One-Dimensional Optimal Transport</p>\\\\n\\\\t</div>\\\\n        <div class=\"tile\" style=\"background-color: white;\"></div>\\\\n        <div class=\"tile\" style=\"background-color: white;\"></div>\\\\n\\\\n</div>\\\\n\\\\n<a name=\"sec_nlp\"></a>\\\\n# II. Multimodal NLP & Commonsense AI\\\\n\\\\n<br>\\\\n\\\\n<div class=\"trow\">\\\\n\\\\t<div class=\"tile\">\\\\n\\\\t\\\\t<h3>ComFact</h3>\\\\n\\\\t\\\\t<a href=\"https://aclanthology.org/2022.findings-emnlp.120/\"><img src=\"./assets/comfact.png\"></a>\\\\n\\\\t\\\\t<h5>\\\\n\\\\t\\\\t\\\\t<a href=\"https://aclanthology.org/2022.findings-emnlp.120/\">[EMNLP]</a>\\\\n\\\\t\\\\t\\\\t<a href=\"https://arxiv.org/abs/2210.12678\">[arXiv]</a>\\\\n\\\\t\\\\t\\\\t<a href=\"https://github.com/epfl-nlp/ComFact\">[code]</a>\\\\n\\\\t\\\\t</h5>\\\\n\\\\t\\\\t<p>ComFact: A Benchmark for Linking Contextual Commonsense Knowledge<br>(EMNLP2022 Findings)</p>\\\\n\\\\t</div>\\\\n        <div class=\"tile\" style=\"background-color: white;\"></div>\\\\n        <div class=\"tile\" style=\"background-color: white;\"></div>\\\\n\\\\n</div>\\\\n\\\\n<a name=\"sec_mct\"></a>\\\\n# III. Music & Cinematic Technologies\\\\n\\\\n<br>\\\\n\\\\n<div class=\"trow\">\\\\n\\\\t<div class=\"tile\">\\\\n\\\\t\\\\t<h3>CLIPSep</h3>\\\\n\\\\t\\\\t<img src=\"./assets/CLIPSep.png\">\\\\n\\\\t\\\\t<h5>\\\\n\\\\t\\\\t\\\\t<a href=\"https://openreview.net/forum?id=H-T3F0dMbyj\">[OpenReview]</a>\\\\n\\\\t\\\\t</h5>\\\\n\\\\t\\\\t<p>CLIPSep: Learning Text-queried Sound Separation with Noisy Unlabeled Videos<br>(ICLR2023)</p>\\\\n\\\\t\\\\t<div class=\"tile_highlight\">ICLR23</div>\\\\n\\\\t</div>\\\\n\\\\t<div class=\"tile\">\\\\n\\\\t\\\\t<h3>Vocal Dereverberation</h3>\\\\n\\\\t\\\\t<img src=\"./assets/dereverb.png\">\\\\n\\\\t\\\\t<h5>\\\\n\\\\t\\\\t\\\\t<a href=\"https://arxiv.org/abs/2211.04124\">[arXiv]</a>\\\\n\\\\t\\\\t\\\\t<a href=\"https://koichi-saito-sony.github.io/unsupervised-vocal-dereverb/\">[demo]</a>\\\\n\\\\t\\\\t</h5>\\\\n\\\\t\\\\t<p>Unsupervised Vocal Dereverberation with Diffusion-based Generative Models<br>(ICASSP23)</p>\\\\n\\\\t\\\\t<div class=\"tile_highlight\">ICASSP23</div>\\\\n\\\\t</div>\\\\t\\\\n\\\\t<div class=\"tile\">\\\\n\\\\t\\\\t<h3>Mixing Style Transfer</h3>\\\\n\\\\t\\\\t<img src=\"./assets/mixstyletransfer.png\">\\\\n\\\\t\\\\t<h5>\\\\n\\\\t\\\\t\\\\t<a href=\"https://arxiv.org/abs/2211.02247\">[arXiv]</a>\\\\n\\\\t\\\\t\\\\t<a href=\"https://github.com/jhtonyKoo/music_mixing_style_transfer\">[code]</a>\\\\n\\\\t\\\\t\\\\t<a href=\"https://jhtonykoo.github.io/MixingStyleTransfer/\">[demo]</a>\\\\n\\\\t\\\\t</h5>\\\\n\\\\t\\\\t<p>Music Mixing Style Transfer: A Contrastive Learning Approach to Disentangle Audio Effects<br>(ICASSP23)</p>\\\\n\\\\t\\\\t<div class=\"tile_highlight\">ICASSP23</div>\\\\n\\\\t</div>\\\\n\\\\t<div class=\"tile\">\\\\n\\\\t\\\\t<h3>Music Transcription</h3>\\\\n\\\\t\\\\t<img src=\"./assets/amt.png\">\\\\n\\\\t\\\\t<h5>\\\\n\\\\t\\\\t\\\\t<a href=\"https://arxiv.org/abs/2210.05148\">[arXiv]</a>\\\\n\\\\t\\\\t\\\\t<a href=\"https://github.com/sony/DiffRoll\">[code]</a>\\\\n\\\\t\\\\t\\\\t<a href=\"https://sony.github.io/DiffRoll/\">[demo]</a>\\\\n\\\\t\\\\t</h5>\\\\n\\\\t\\\\t<p>DiffRoll: Diffusion-based Generative Music Transcription with Unsupervised Pretraining Capability<br>(ICASSP23)</p>\\\\n\\\\t\\\\t<div class=\"tile_highlight\">ICASSP23</div>\\\\n\\\\t</div>\\\\n\\\\t<div class=\"tile\">\\\\n\\\\t\\\\t<h3>Singing Voice Vocoder</h3>\\\\n\\\\t\\\\t<img src=\"./assets/vocoder.png\">\\\\n\\\\t\\\\t<h5>\\\\n\\\\t\\\\t\\\\t<a href=\"https://arxiv.org/abs/2210.07508\">[arXiv]</a>\\\\n\\\\t\\\\t\\\\t<a href=\"https://t-naoya.github.io/hdm/\">[demo]</a>\\\\n\\\\t\\\\t</h5>\\\\n\\\\t\\\\t<p>Hierarchical Diffusion Models for Singing Voice Neural Vocoder<br>(ICASSP23)</p>\\\\n\\\\t\\\\t<div class=\"tile_highlight\">ICASSP23</div>\\\\n\\\\t</div>\\\\n\\\\t<div class=\"tile\">\\\\n\\\\t\\\\t<h3>Distortion Effect Removal</h3>\\\\n\\\\t\\\\t<img src=\"./assets/deeffect.png\">\\\\n\\\\t\\\\t<h5>\\\\n\\\\t\\\\t\\\\t<a href=\"https://ismir2022program.ismir.net/poster_113.html\">[poster]</a>\\\\n\\\\t\\\\t\\\\t<a href=\"https://arxiv.org/abs/2202.01664\">[arXiv]</a>\\\\n\\\\t\\\\t\\\\t<a href=\"https://joimort.github.io/distortionremoval/\">[demo]</a>\\\\n\\\\t\\\\t</h5>\\\\n\\\\t\\\\t<p>Distortion Audio Effects: Learning How to Recover the Clean Signal<br>(ISMIR22)</p>\\\\n\\\\t</div>\\\\n\\\\t<div class=\"tile\">\\\\n\\\\t\\\\t<h3>Automatic Music Mixing</h3>\\\\n\\\\t\\\\t<img src=\"./assets/automix.png\">\\\\n\\\\t\\\\t<h5>\\\\n\\\\t\\\\t\\\\t<a href=\"https://ismir2022program.ismir.net/poster_11.html\">[poster]</a>\\\\n\\\\t\\\\t\\\\t<a href=\"https://arxiv.org/abs/2208.11428\">[arXiv]</a>\\\\t\\\\t\\\\t\\\\n\\\\t\\\\t\\\\t<a href=\"https://github.com/sony/fxnorm-automix\">[code]</a>\\\\n\\\\t\\\\t\\\\t<a href=\"https://marco-martinez-sony.github.io/FxNorm-automix/\">[demo]</a>\\\\n\\\\t\\\\t</h5>\\\\n\\\\t\\\\t<p>Automatic Music Mixing with Deep Learning and Out-of-Domain Data<br>(ISMIR22)</p>\\\\n\\\\t</div>\\\\t\\\\n\\\\t<div class=\"tile\">\\\\n\\\\t\\\\t<h3>Sound Separation</h3>\\\\n\\\\t\\\\t<a href=\"https://ieeexplore.ieee.org/document/9746317\"><img src=\"./assets/srcsep.png\"></a>\\\\n\\\\t\\\\t<h5>\\\\n\\\\t\\\\t\\\\t<a href=\"https://ieeexplore.ieee.org/document/9746317\">[IEEE]</a>\\\\n\\\\t\\\\t</h5>\\\\n\\\\t\\\\t<p>Music Source Separation with Deep Equilibrium Models<br>(ICASSP22)</p>\\\\n\\\\t</div>\\\\n\\\\t<div class=\"tile\">\\\\n\\\\t\\\\t<h3>Automatic DJ Transition</h3>\\\\n\\\\t\\\\t<img src=\"./assets/djtransgan.png\">\\\\n\\\\t\\\\t<h5>\\\\n\\\\t\\\\t\\\\t<a href=\"https://arxiv.org/abs/2110.06525\">[arXiv]</a>\\\\n\\\\t\\\\t\\\\t<a href=\"https://github.com/ChenPaulYu/DJtransGAN\">[code]</a>\\\\n\\\\t\\\\t\\\\t<a href=\"https://paulyuchen.com/djtransgan-icassp2022/\">[demo]</a>\\\\n\\\\t\\\\t</h5>\\\\n\\\\t\\\\t<p>Automatic DJ Transitions with Differentiable Audio Effects and Generative Adversarial Networks<br>(ICASSP22)</p>\\\\n\\\\t</div>\\\\n\\\\t<div class=\"tile\">\\\\n\\\\t\\\\t<h3>Singing Voice Conversion</h3>\\\\n\\\\t\\\\t<img src=\"./assets/svc.png\">\\\\n\\\\t\\\\t<h5>\\\\n\\\\t\\\\t\\\\t<a href=\"https://arxiv.org/abs/2210.11096\">[arXiv]</a>\\\\n\\\\t\\\\t\\\\t<a href=\"https://t-naoya.github.io/rosvc/\">[demo]</a>\\\\n\\\\t\\\\t</h5>\\\\n\\\\t\\\\t<p>Robust One-Shot Singing Voice Conversion</p>\\\\n\\\\t</div>\\\\n\\\\t<div class=\"tile\">\\\\n\\\\t\\\\t<h3>Sound Separation</h3>\\\\n\\\\t\\\\t<a href=\"https://www.youtube.com/watch?v=EWYxJGmw0Ng\"><img src=\"./assets/enoch_arden.jpg\"></a>\\\\n\\\\t\\\\t<h5>\\\\n\\\\t\\\\t\\\\t<a href=\"https://www.youtube.com/watch?v=EWYxJGmw0Ng\">[video]</a>\\\\n\\\\t\\\\t\\\\t<a href=\"https://www.sony.com/en/SonyInfo/technology/stories/AI_Sound_Separation/\">[site]</a>\\\\n\\\\t\\\\t</h5>\\\\n\\\\t\\\\t<p>Glenn Gould and Kanji Ishimaru 2021: A collaboration with AI Sound Separation after 60 years</p>\\\\n\\\\t</div>\\\\n\\\\t<div class=\"tile\">\\\\n\\\\t\\\\t<h3>MDX21</h3>\\\\n\\\\t\\\\t<a href=\"https://mdx-workshop.github.io/\"><img src=\"./assets/MDX.png\"></a>\\\\n\\\\t\\\\t<h5>\\\\n\\\\t\\\\t\\\\t<a href=\"https://mdx-workshop.github.io/\">[site]</a>\\\\n\\\\t\\\\t\\\\t<a href=\"https://www.frontiersin.org/articles/10.3389/frsip.2021.808395/full\">[frontiers]</a>\\\\n\\\\t\\\\t</h5>\\\\n\\\\t\\\\t<p>Music Demixing Challenge 2021</p>\\\\n\\\\t</div>\\\\t\\\\t\\\\n\\\\t<div class=\"tile\">\\\\n\\\\t\\\\t<h3>DCASE Challenge</h3>\\\\n\\\\t\\\\t<img src=\"./assets/DCASE.png\">\\\\n\\\\t\\\\t<h5>\\\\n\\\\t\\\\t\\\\t<a href=\"https://dcase.community/challenge2023/#sound-event-localization-and-detection-evaluated-in-real-spatial-sound-scenes\">[DCASE Challenge2023]</a>\\\\n\\\\t\\\\t</h5>\\\\n\\\\t\\\\t<p>Sound Event Localization and Detection Evaluated in Real Spatial Sound Scenes</p>\\\\n\\\\t</div>\\\\t\\\\n\\\\t<div class=\"tile\">\\\\n\\\\t\\\\t<h3>Sound Event Localization and Detection</h3>\\\\n\\\\t\\\\t<img src=\"./assets/ACCDOA.png\">\\\\n\\\\t\\\\t<h5>\\\\n\\\\t\\\\t\\\\t<a href=\"https://arxiv.org/abs/2110.07124\">[arXiv]</a>\\\\n\\\\t\\\\t</h5>\\\\n\\\\t\\\\t<p>Multi-ACCDOA: Localizing and Detecting Overlapping Sounds from the Same Class with Auxiliary Duplicating Permutation Invariant Training (ICASSP2022)</p>\\\\n\\\\t</div>\\\\t\\\\n\\\\t<div class=\"tile\">\\\\n\\\\t\\\\t<h3>Automatic Music Tagging</h3>\\\\n\\\\t\\\\t<img src=\"./assets/ResAtt.png\">\\\\n\\\\t\\\\t<h5>\\\\n\\\\t\\\\t\\\\t<a href=\"https://arxiv.org/abs/2302.08136\">[arXiv]</a>\\\\n\\\\t\\\\t</h5>\\\\n\\\\t\\\\t<p>An Attention-based Approach To Hierarchical Multi-label Music Instrument Classification<br>(ICASSP2023)</p>\\\\n\\\\t\\\\t<div class=\"tile_highlight\">ICASSP23</div>\\\\n\\\\t</div>\\\\n</div>\\\\n\\\\n### Contact\\\\n<h5 align=\"left\"> Yuki Mitsufuji (yuhki.mitsufuji@sony.com) </h5>\\\\n\\''"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(repos[10]['readme'].decoded_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240b3187-75ae-4cb2-9982-91c52dcd4768",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
