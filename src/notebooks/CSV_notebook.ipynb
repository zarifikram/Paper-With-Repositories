{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4628d59f-a9ad-4f35-8c71-d235057c4d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current working dir/Volumes/Zarif/myStuff/research/repoPaper/src\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "exec(open(\"init_notebook.py\").read())\n",
    "from DataTools import DataTools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dab8b8c1-7b56-4d65-8835-55b177c1fc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "993d0564-9395-4289-a2d5-1a1d4a890744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CID</th>\n",
       "      <th>Company</th>\n",
       "      <th>Year</th>\n",
       "      <th>Source title</th>\n",
       "      <th>EID</th>\n",
       "      <th>DOI</th>\n",
       "      <th>Title</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Affiliations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1693.0</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>Lecture Notes in Computer Science (including s...</td>\n",
       "      <td>2-s2.0-35048830214</td>\n",
       "      <td>10.1007/3-540-44634-6_13</td>\n",
       "      <td>Seller-focused algorithms for online auctioning</td>\n",
       "      <td>In this paper we provide an algorithmic approa...</td>\n",
       "      <td>Dept of Computer Science, Johns Hopkins Univer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1693.0</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>2003.0</td>\n",
       "      <td>International Conference on Information and Kn...</td>\n",
       "      <td>2-s2.0-7244220821</td>\n",
       "      <td>10.1145/956863.956924</td>\n",
       "      <td>Efficient multi-way text categorization via ge...</td>\n",
       "      <td>Text categorization is an important research a...</td>\n",
       "      <td>Computer Science Dept., University of Rocheste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1693.0</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>2003.0</td>\n",
       "      <td>Annual ACM Symposium on Parallel Algorithms an...</td>\n",
       "      <td>2-s2.0-0038717549</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Efficient galois field arithmetic on SIMD arch...</td>\n",
       "      <td>The use of efficient Galois Field Arithmetic o...</td>\n",
       "      <td>Project CODES, INRIA Rocquencourt, Le Chesnay,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1693.0</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>2003.0</td>\n",
       "      <td>Proceedings of the ASIST Annual Meeting</td>\n",
       "      <td>2-s2.0-34247109193</td>\n",
       "      <td>10.1002/meet.1450400184</td>\n",
       "      <td>The bibliomining process: Data warehousing and...</td>\n",
       "      <td>Bibliomining is the combination of data mining...</td>\n",
       "      <td>Syracuse University, School of Information Stu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1693.0</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>2003.0</td>\n",
       "      <td>Proceedings - IEEE International Conference on...</td>\n",
       "      <td>2-s2.0-78149338227</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Using discriminant analysis for multi-class cl...</td>\n",
       "      <td>Discriminant analysis is known to learn discri...</td>\n",
       "      <td>Department of Computer Science, University of ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CID Company    Year                                       Source title  \\\n",
       "0  1693.0  Amazon  2001.0  Lecture Notes in Computer Science (including s...   \n",
       "1  1693.0  Amazon  2003.0  International Conference on Information and Kn...   \n",
       "2  1693.0  Amazon  2003.0  Annual ACM Symposium on Parallel Algorithms an...   \n",
       "3  1693.0  Amazon  2003.0            Proceedings of the ASIST Annual Meeting   \n",
       "4  1693.0  Amazon  2003.0  Proceedings - IEEE International Conference on...   \n",
       "\n",
       "                  EID                       DOI  \\\n",
       "0  2-s2.0-35048830214  10.1007/3-540-44634-6_13   \n",
       "1   2-s2.0-7244220821     10.1145/956863.956924   \n",
       "2   2-s2.0-0038717549                       NaN   \n",
       "3  2-s2.0-34247109193   10.1002/meet.1450400184   \n",
       "4  2-s2.0-78149338227                       NaN   \n",
       "\n",
       "                                               Title  \\\n",
       "0    Seller-focused algorithms for online auctioning   \n",
       "1  Efficient multi-way text categorization via ge...   \n",
       "2  Efficient galois field arithmetic on SIMD arch...   \n",
       "3  The bibliomining process: Data warehousing and...   \n",
       "4  Using discriminant analysis for multi-class cl...   \n",
       "\n",
       "                                            Abstract  \\\n",
       "0  In this paper we provide an algorithmic approa...   \n",
       "1  Text categorization is an important research a...   \n",
       "2  The use of efficient Galois Field Arithmetic o...   \n",
       "3  Bibliomining is the combination of data mining...   \n",
       "4  Discriminant analysis is known to learn discri...   \n",
       "\n",
       "                                        Affiliations  \n",
       "0  Dept of Computer Science, Johns Hopkins Univer...  \n",
       "1  Computer Science Dept., University of Rocheste...  \n",
       "2  Project CODES, INRIA Rocquencourt, Le Chesnay,...  \n",
       "3  Syracuse University, School of Information Stu...  \n",
       "4  Department of Computer Science, University of ...  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"AppleAmazon.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "619fd86f-a7f8-4534-98fd-1237a74b96e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "somePapers = df.Title.to_list()[2500:-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0e3a4cdb-428d-4947-8d38-64af41d72086",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 857/857 [16:40<00:00,  1.17s/it]\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "links = []\n",
    "for paper in tqdm(somePapers):\n",
    "    if type(paper) != str:\n",
    "        links.append(\"\")\n",
    "        continue\n",
    "    paperWithoutSpace = paper.replace(\" \", \"+\") \n",
    "    link = f\"https://www.google.com/search?q=github+{paperWithoutSpace}\"\n",
    "    response = requests.get(link)\n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    github_link = \"\"\n",
    "    for link in soup.find_all(\"a\"):\n",
    "        href = link.get(\"href\")\n",
    "        if href and \"https://github.com\" in href:\n",
    "            startIndex = href.find(\"https://github.com\")\n",
    "            endIndex = href.find(\"&\")\n",
    "            href = href[startIndex : endIndex]\n",
    "            github_link = href\n",
    "            break\n",
    "        \n",
    "    links.append(github_link)\n",
    "            \n",
    "            \n",
    "df[\"paper\"] = somePapers\n",
    "df[\"github_link\"] = links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c313c184-7a07-46ce-9a2b-80edf7ffc917",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"PaperLink.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f64795-1b9c-42e6-80ee-eff7331c6a11",
   "metadata": {},
   "source": [
    "### Trying a new way. Finding all repos in Amazon "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a808bca-07df-4fdd-bf9d-a9f4ec0ac533",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "# import re\n",
    "\n",
    "PAT = \"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\"\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {PAT}\",\n",
    "    # \"Accept\": \"application/vnd.github.v3+json\"\n",
    "}\n",
    "\n",
    "API_URL = \"https://api.github.com\"\n",
    "\n",
    "user = \"microsoft\"\n",
    "repos_url = f\"{API_URL}/users/{user}/repos\"\n",
    "\n",
    "response = requests.get(repos_url, headers = headers)\n",
    "repos = response.json()\n",
    "\n",
    "# print(repos)\n",
    "for repo in repos:\n",
    "    print(f\"Trying for \", {repo[\"name\"]})\n",
    "    readme_url = f\"{API_URL}/repos/{user}/{repo['name']}/readme\"\n",
    "    getPaperNameFromLink(readme_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dabc0630-7a13-4d06-bd0b-63cd5dc3ac38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authentication failed.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Define the Github API URL and your PAT\n",
    "API_URL = \"https://api.github.com\"\n",
    "PAT = \"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\"\n",
    "\n",
    "# Define the headers with your PAT\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {PAT}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"\n",
    "}\n",
    "\n",
    "# Make a test request to the Github API to verify authentication\n",
    "user_url = f\"{API_URL}/user\"\n",
    "response = requests.get(user_url, headers=headers)\n",
    "\n",
    "# Print the response to verify authentication\n",
    "if response.status_code == 200:\n",
    "    user_data = response.json()\n",
    "    print(f\"Authentication successful for user: {user_data['login']}\")\n",
    "else:\n",
    "    print(\"Authentication failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "4767d203-f28b-4aec-b29d-e152941bd9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPaperNameFromLink(link):\n",
    "    response = requests.get(link, headers = headers)\n",
    "    readme = response.json()\n",
    "    # print(readme)\n",
    "    if readme and \"content\" in readme:\n",
    "        decoded_content = base64.b64decode(readme[\"content\"]).decode(\"utf-8\")\n",
    "        # print(decoded_content)\n",
    "        while \"title\" in decoded_content:\n",
    "            ind = decoded_content.find(\"title\")\n",
    "            substr = decoded_content[ind : ]\n",
    "            s = substr.find(\"{\")\n",
    "            e = substr.find(\"}\")\n",
    "            decoded_content = substr[e:]              \n",
    "            print(substr[s + 1:e])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "014307d2-d4ba-46fe-baba-2adf5de88ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Symbolic Discovery of Optimization Algorithms\n",
      "Triton: an intermediate language and compiler for tiled neural network computations\n"
     ]
    }
   ],
   "source": [
    "DataTools.getPaperNameFromLink(\"https://api.github.com/repos/lucidrains/lion-pytorch/readme\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24785514-ba58-491f-88d2-c677a1c82a0b",
   "metadata": {},
   "source": [
    "### This will load the repos and some basic info and post it in pandas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "155e9a25-7730-4371-bd51-1f4d64743724",
   "metadata": {},
   "outputs": [],
   "source": [
    "repoNames = DataTools.getAllReposFromUser('amzn')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cb34f3-c85f-4770-bce6-6a10ade580b5",
   "metadata": {},
   "source": [
    "### This will save all repos and save them in a pickle file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "00f291ee-ef5f-4898-86a6-1b1011472069",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                         | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to fetch from page 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██████████████████████▌                                                                                          | 1/5 [00:00<00:03,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to fetch from page 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|█████████████████████████████████████████████▏                                                                   | 2/5 [00:01<00:02,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to fetch from page 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [00:02<00:01,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to fetch from page 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|██████████████████████████████████████████████████████████████████████████████████████████▍                      | 4/5 [00:03<00:00,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to fetch from page 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:04<00:00,  1.08it/s]\n"
     ]
    }
   ],
   "source": [
    "owner = \"amzn\"\n",
    "DataTools.saveAllReposFromUser(owner)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13168c8-8793-44e7-988f-c8e5589326f2",
   "metadata": {},
   "source": [
    "### Load the repo info  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "aa203b5e-bac1-4f0c-8a42-407fbce1b84a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening for amzn\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "print(f\"Opening for {owner}\")\n",
    "with open(f\"{owner}Repos\", 'rb') as handle:\n",
    "    repos = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab02c01f-ec52-4f7b-89c0-30ea5d470c09",
   "metadata": {},
   "source": [
    "### loading the first repo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b9725796-d389-4f09-a0b0-b260575cb8ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'amzn/amazon-hub-support'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repos[10].full_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff33eaa5-d08a-4d63-a9d1-1d40de603522",
   "metadata": {},
   "source": [
    "### from repos build a dataframe. (check getreadme to retrieve as well)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "72f3df4e-989e-4fb8-a896-6d5e0afb5863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting dataframe for amzn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 130/130 [01:06<00:00,  1.97it/s]\n"
     ]
    }
   ],
   "source": [
    "df = DataTools.getDataFrameFromRepoList(repos, owner , True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437d8dfb-76be-4f61-b16a-2374f726d915",
   "metadata": {},
   "source": [
    "###  Saving data in csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a4b3dbc4-4fd2-4253-af60-2977648a1591",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f\"{owner}WithReadme.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101b3839-4e9c-46ba-b24a-0d62987da289",
   "metadata": {},
   "source": [
    "### Varifying username with repos' readme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a1e34652-7155-43b8-9e3a-acabf78e33ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amazon-hub-support\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'---\\nlayout: default\\npage: root\\n\\n---\\n\\n## Amazon hub support digital material\\n\\nWelcome to Amazon Hub support digital training. \\n'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id = 10\n",
    "print(df.iloc[id][\"name\"])\n",
    "DataTools.getDecodedReadme(df.loc[id].readme_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c002d7fa-d8be-4831-91b6-24a8672bd70d",
   "metadata": {},
   "source": [
    "### Loading a csv repo for readme processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "16f635ed-5847-436b-a824-e9476d73a190",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"amznWithReadme.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "696a24bf-f0ae-453b-a5ca-10771cbc0a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found in 35\n",
      "found in 37\n",
      "found in 41\n",
      "found in 46\n",
      "found in 48\n",
      "found in 49\n",
      "found in 53\n",
      "found in 54\n",
      "found in 61\n",
      "found in 62\n",
      "found in 75\n",
      "found in 76\n",
      "found in 82\n",
      "found in 86\n",
      "found in 90\n",
      "found in 94\n",
      "found in 96\n",
      "found in 97\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    decoded_readme = DataTools.getDecodedReadme(df.iloc[i].readme_encoded)\n",
    "    if \"arxiv\" in decoded_readme or \"@inproceedings\" in decoded_readme or \"@article\" in decoded_readme:\n",
    "        print(f\"found in {i}\")\n",
    "        # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "fe50635e-763c-4b6d-bc2a-7860a14072bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_readme = DataTools.getDecodedReadme(df.iloc[53].readme_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "45ca7600-6996-4855-b19c-fd0583350463",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Debiased balanced interleaving at Amazon Search'"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# decoded_readme[decoded_readme.find(\"@inproceedings\") : ]\n",
    "titleInd = decoded_readme.find(\"title\")\n",
    "braces = decoded_readme.find(\"{\", titleInd)\n",
    "newlineInd = decoded_readme.find(\"}\", braces)\n",
    "decoded_readme[braces + 1 : newlineInd]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "08fcaced-383c-4ca3-bfc5-137d54345c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{SDR}: Efficient Neural Re-ranking using Succinct DocumentRepresentation 35\n",
      "Named Entity Recognition with Small Strongly Labeled and Large Weakly Labeled Data 37\n",
      "Learning to Bid with AuctionGym 41\n",
      "Efficient Learning on Point Clouds With Basis Point Sets 46\n",
      "ScrabbleGAN: Semi-Supervised Varying Length Handwritten Text Generation 49\n",
      "Debiased balanced interleaving at Amazon Search 53\n",
      "Automatic Discovery of Privacy--Utility Pareto Fronts 54\n",
      "A First Look: Towards Explainable {T}ext{VQA} Models via Visual and Textual Explanations 61\n",
      "Learning Attribute-driven Disentangled Representations for Interactive Fashion Retrieval 62\n",
      "Misspelling Detection from Noisy Product Images 75\n",
      "Revamping Cross-Modal Recipe Retrieval with Hierarchical Transformers and Self-supervised Learning 76\n",
      "Learning Multimodal Affinities for Textual Editing in Images 86\n",
      "Answering Ambiguous Questions through Generative Evidence Fusion and Round-Trip Prediction 96\n",
      "RETE: Retrieval-Enhanced Temporal Event Forecasting on Unified Query Product Evolutionary Graph 97\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    decoded_readme = DataTools.getDecodedReadme(df.iloc[i].readme_encoded)\n",
    "    if \"@inproceedings\" in decoded_readme or \"@article\" in decoded_readme:\n",
    "        titleInd = decoded_readme.find(\"title\")\n",
    "        braceStart = decoded_readme.find(\"{\", titleInd, titleInd + 50)\n",
    "        braceEnd = decoded_readme.find(\"}\", braceStart + 1)\n",
    "        quoteStart = decoded_readme.find(\"\\\"\", titleInd, titleInd + 50)\n",
    "        quoteEnd = decoded_readme.find(\"\\\"\", quoteStart + 1)\n",
    "        \n",
    "        # print(f\" brace = {braceStart} quote = {quoteStart}\")     \n",
    "        # print(f\" braceend = {braceEnd} quoteEnd = {quoteEnd}\")\n",
    "        if quoteStart != -1: \n",
    "            print(decoded_readme[quoteStart + 1 : quoteEnd], i)\n",
    "        elif braceStart != -1:\n",
    "            print(decoded_readme[braceStart + 1 : braceEnd], i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f40921c-9b51-4329-8844-331bf065f3b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
