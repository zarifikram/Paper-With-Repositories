{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4628d59f-a9ad-4f35-8c71-d235057c4d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from DataTools import DataTools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "993d0564-9395-4289-a2d5-1a1d4a890744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CID</th>\n",
       "      <th>Company</th>\n",
       "      <th>Year</th>\n",
       "      <th>Source title</th>\n",
       "      <th>EID</th>\n",
       "      <th>DOI</th>\n",
       "      <th>Title</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Affiliations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1693.0</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>Lecture Notes in Computer Science (including s...</td>\n",
       "      <td>2-s2.0-35048830214</td>\n",
       "      <td>10.1007/3-540-44634-6_13</td>\n",
       "      <td>Seller-focused algorithms for online auctioning</td>\n",
       "      <td>In this paper we provide an algorithmic approa...</td>\n",
       "      <td>Dept of Computer Science, Johns Hopkins Univer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1693.0</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>2003.0</td>\n",
       "      <td>International Conference on Information and Kn...</td>\n",
       "      <td>2-s2.0-7244220821</td>\n",
       "      <td>10.1145/956863.956924</td>\n",
       "      <td>Efficient multi-way text categorization via ge...</td>\n",
       "      <td>Text categorization is an important research a...</td>\n",
       "      <td>Computer Science Dept., University of Rocheste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1693.0</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>2003.0</td>\n",
       "      <td>Annual ACM Symposium on Parallel Algorithms an...</td>\n",
       "      <td>2-s2.0-0038717549</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Efficient galois field arithmetic on SIMD arch...</td>\n",
       "      <td>The use of efficient Galois Field Arithmetic o...</td>\n",
       "      <td>Project CODES, INRIA Rocquencourt, Le Chesnay,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1693.0</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>2003.0</td>\n",
       "      <td>Proceedings of the ASIST Annual Meeting</td>\n",
       "      <td>2-s2.0-34247109193</td>\n",
       "      <td>10.1002/meet.1450400184</td>\n",
       "      <td>The bibliomining process: Data warehousing and...</td>\n",
       "      <td>Bibliomining is the combination of data mining...</td>\n",
       "      <td>Syracuse University, School of Information Stu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1693.0</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>2003.0</td>\n",
       "      <td>Proceedings - IEEE International Conference on...</td>\n",
       "      <td>2-s2.0-78149338227</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Using discriminant analysis for multi-class cl...</td>\n",
       "      <td>Discriminant analysis is known to learn discri...</td>\n",
       "      <td>Department of Computer Science, University of ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CID Company    Year                                       Source title  \\\n",
       "0  1693.0  Amazon  2001.0  Lecture Notes in Computer Science (including s...   \n",
       "1  1693.0  Amazon  2003.0  International Conference on Information and Kn...   \n",
       "2  1693.0  Amazon  2003.0  Annual ACM Symposium on Parallel Algorithms an...   \n",
       "3  1693.0  Amazon  2003.0            Proceedings of the ASIST Annual Meeting   \n",
       "4  1693.0  Amazon  2003.0  Proceedings - IEEE International Conference on...   \n",
       "\n",
       "                  EID                       DOI  \\\n",
       "0  2-s2.0-35048830214  10.1007/3-540-44634-6_13   \n",
       "1   2-s2.0-7244220821     10.1145/956863.956924   \n",
       "2   2-s2.0-0038717549                       NaN   \n",
       "3  2-s2.0-34247109193   10.1002/meet.1450400184   \n",
       "4  2-s2.0-78149338227                       NaN   \n",
       "\n",
       "                                               Title  \\\n",
       "0    Seller-focused algorithms for online auctioning   \n",
       "1  Efficient multi-way text categorization via ge...   \n",
       "2  Efficient galois field arithmetic on SIMD arch...   \n",
       "3  The bibliomining process: Data warehousing and...   \n",
       "4  Using discriminant analysis for multi-class cl...   \n",
       "\n",
       "                                            Abstract  \\\n",
       "0  In this paper we provide an algorithmic approa...   \n",
       "1  Text categorization is an important research a...   \n",
       "2  The use of efficient Galois Field Arithmetic o...   \n",
       "3  Bibliomining is the combination of data mining...   \n",
       "4  Discriminant analysis is known to learn discri...   \n",
       "\n",
       "                                        Affiliations  \n",
       "0  Dept of Computer Science, Johns Hopkins Univer...  \n",
       "1  Computer Science Dept., University of Rocheste...  \n",
       "2  Project CODES, INRIA Rocquencourt, Le Chesnay,...  \n",
       "3  Syracuse University, School of Information Stu...  \n",
       "4  Department of Computer Science, University of ...  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"AppleAmazon.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "619fd86f-a7f8-4534-98fd-1237a74b96e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "somePapers = df.Title.to_list()[2500:-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0e3a4cdb-428d-4947-8d38-64af41d72086",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 857/857 [16:40<00:00,  1.17s/it]\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "links = []\n",
    "for paper in tqdm(somePapers):\n",
    "    if type(paper) != str:\n",
    "        links.append(\"\")\n",
    "        continue\n",
    "    paperWithoutSpace = paper.replace(\" \", \"+\") \n",
    "    link = f\"https://www.google.com/search?q=github+{paperWithoutSpace}\"\n",
    "    response = requests.get(link)\n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    github_link = \"\"\n",
    "    for link in soup.find_all(\"a\"):\n",
    "        href = link.get(\"href\")\n",
    "        if href and \"https://github.com\" in href:\n",
    "            startIndex = href.find(\"https://github.com\")\n",
    "            endIndex = href.find(\"&\")\n",
    "            href = href[startIndex : endIndex]\n",
    "            github_link = href\n",
    "            break\n",
    "        \n",
    "    links.append(github_link)\n",
    "            \n",
    "            \n",
    "df[\"paper\"] = somePapers\n",
    "df[\"github_link\"] = links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c313c184-7a07-46ce-9a2b-80edf7ffc917",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"PaperLink.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f64795-1b9c-42e6-80ee-eff7331c6a11",
   "metadata": {},
   "source": [
    "### Trying a new way. Finding all repos in Amazon "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a808bca-07df-4fdd-bf9d-a9f4ec0ac533",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "# import re\n",
    "\n",
    "PAT = \"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\"\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {PAT}\",\n",
    "    # \"Accept\": \"application/vnd.github.v3+json\"\n",
    "}\n",
    "\n",
    "API_URL = \"https://api.github.com\"\n",
    "\n",
    "user = \"microsoft\"\n",
    "repos_url = f\"{API_URL}/users/{user}/repos\"\n",
    "\n",
    "response = requests.get(repos_url, headers = headers)\n",
    "repos = response.json()\n",
    "\n",
    "# print(repos)\n",
    "for repo in repos:\n",
    "    print(f\"Trying for \", {repo[\"name\"]})\n",
    "    readme_url = f\"{API_URL}/repos/{user}/{repo['name']}/readme\"\n",
    "    getPaperNameFromLink(readme_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "dabc0630-7a13-4d06-bd0b-63cd5dc3ac38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authentication successful for user: zarifikram\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Define the Github API URL and your PAT\n",
    "API_URL = \"https://api.github.com\"\n",
    "PAT = \"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\"\n",
    "\n",
    "# Define the headers with your PAT\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {PAT}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"\n",
    "}\n",
    "\n",
    "# Make a test request to the Github API to verify authentication\n",
    "user_url = f\"{API_URL}/user\"\n",
    "response = requests.get(user_url, headers=headers)\n",
    "\n",
    "# Print the response to verify authentication\n",
    "if response.status_code == 200:\n",
    "    user_data = response.json()\n",
    "    print(f\"Authentication successful for user: {user_data['login']}\")\n",
    "else:\n",
    "    print(\"Authentication failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "4767d203-f28b-4aec-b29d-e152941bd9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPaperNameFromLink(link):\n",
    "    response = requests.get(link, headers = headers)\n",
    "    readme = response.json()\n",
    "    # print(readme)\n",
    "    if readme and \"content\" in readme:\n",
    "        decoded_content = base64.b64decode(readme[\"content\"]).decode(\"utf-8\")\n",
    "        # print(decoded_content)\n",
    "        while \"title\" in decoded_content:\n",
    "            ind = decoded_content.find(\"title\")\n",
    "            substr = decoded_content[ind : ]\n",
    "            s = substr.find(\"{\")\n",
    "            e = substr.find(\"}\")\n",
    "            decoded_content = substr[e:]              \n",
    "            print(substr[s + 1:e])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "014307d2-d4ba-46fe-baba-2adf5de88ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Symbolic Discovery of Optimization Algorithms\n",
      "Triton: an intermediate language and compiler for tiled neural network computations\n"
     ]
    }
   ],
   "source": [
    "DataTools.getPaperNameFromLink(\"https://api.github.com/repos/lucidrains/lion-pytorch/readme\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "155e9a25-7730-4371-bd51-1f4d64743724",
   "metadata": {},
   "outputs": [],
   "source": [
    "from DataTools import DataTools\n",
    "# repoNames = DataTools.getAllReposFromUser('amzn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00f291ee-ef5f-4898-86a6-1b1011472069",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                         | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to fetch from page 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|████████████████▏                                                                                                | 1/7 [00:00<00:05,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to fetch from page 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|████████████████████████████████▎                                                                                | 2/7 [00:01<00:04,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to fetch from page 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████████████████████████████████████████████████▍                                                                | 3/7 [00:02<00:03,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to fetch from page 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|████████████████████████████████████████████████████████████████▌                                                | 4/7 [00:03<00:02,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to fetch from page 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|████████████████████████████████████████████████████████████████████████████████▋                                | 5/7 [00:04<00:01,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to fetch from page 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████████████████████████████████████████████████████████████████████████████████████████████▊                | 6/7 [00:05<00:00,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to fetch from page 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:05<00:00,  1.24it/s]\n"
     ]
    }
   ],
   "source": [
    "DataTools.saveAllReposFromUser('apple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa203b5e-bac1-4f0c-8a42-407fbce1b84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('amznRepos', 'rb') as handle:\n",
    "    repos = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72f3df4e-989e-4fb8-a896-6d5e0afb5863",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = DataTools.getDataFrameFromRepoList(repos, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1e34652-7155-43b8-9e3a-acabf78e33ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"amznRepositories.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e45631b-ce00-4e40-9ace-1f8e563f35ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
